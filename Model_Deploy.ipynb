{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3MFL7rk80",
        "outputId": "8ea0dae6-8600-467d-8373-759451c32625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.89.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette==0.22.0\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from fastapi) (1.10.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi) (4.4.0)\n",
            "Collecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok) (6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi) (2.10)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19792 sha256=670ab308c9d3e6b458794ea91a902b84e553220f992c2e1743f138df68800cea\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/f2/70/526da675d32f17577ec47ac4c663084efe39d47c826b6c3bb1\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: sniffio, pyngrok, nest-asyncio, h11, uvicorn, anyio, starlette, fastapi\n",
            "Successfully installed anyio-3.6.2 fastapi-0.89.0 h11-0.14.0 nest-asyncio-1.5.6 pyngrok-5.2.1 sniffio-1.3.0 starlette-0.22.0 uvicorn-0.20.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from python-multipart) (1.15.0)\n",
            "Building wheels for collected packages: python-multipart\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=117c5d1132cf1cca032d851abe2319283923338d4ec4a2ee0a06daae297dd2db\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built python-multipart\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn\n",
        "!pip install Pillow\n",
        "!pip install python-multipart\n",
        "#import re\n",
        "#import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct31ozOeqdc4"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get install python3.7\n",
        "# !sudo apt-get update -y\n",
        "# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1 \n",
        "# !sudo update-alternatives --config python3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pickle\n",
        "from tensorflow.keras.applications import densenet\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Conv2D, Concatenate, Flatten, Add, Dropout, GRU\n",
        "import random\n",
        "import datetime\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "from fastapi import FastAPI\n",
        "from fastapi import UploadFile, File\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from PIL import Image\n",
        "from fastapi import Request, FastAPI\n",
        "\n",
        "import glob"
      ],
      "metadata": {
        "id": "3fE_KedSgeRy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')#mounting google drive "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMNNZsDbgef0",
        "outputId": "73c22c5d-cfb9-4746-d41e-4ca43363ae68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv('/content/drive/MyDrive/FYP2/Train_Data.csv')\n",
        "cv_dataset = pd.read_csv('/content/drive/MyDrive/FYP2/CV_Data.csv')\n",
        "test_dataset = pd.read_csv('/content/drive/MyDrive/FYP2/Test_Data.csv')"
      ],
      "metadata": {
        "id": "E0gYM8eog6Cr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_size = 153   # Max length\n",
        "X_train, X_test, X_cv = train_dataset['Person_id'], test_dataset['Person_id'], cv_dataset['Person_id'][:546]\n",
        "y_train, y_test, y_cv = train_dataset['Report'], test_dataset['Report'], cv_dataset['Report'][:546]\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(y_train.values)\n",
        "f = open('/content/drive/MyDrive/FYP2/Image_features_attention2.pickle','rb') # contains the features from chexNet\n",
        "Xnet_Features = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "chex = densenet.DenseNet121(include_top=False, weights = None,   input_shape=(224,224,3), pooling=\"avg\")\n",
        "X = chex.output\n",
        "X = Dense(14, activation=\"sigmoid\", name=\"predictions\")(X)\n",
        "model = Model(inputs=chex.input, outputs=X)\n",
        "model.load_weights('/content/drive/MyDrive/FYP2/brucechou1983_CheXNet_Keras_0.3.0_weights.h5')\n",
        "chexnet = Model(inputs = model.input, outputs = model.layers[-2].output)\n",
        "#chexnet.summary()\n",
        "\n",
        "\n",
        "f = open('/content/drive/MyDrive/FYP2/glove.840B.300d.pkl','rb') # 300d glove vectors  \n",
        "glove_vectors = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index.keys()) + 1\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size,300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in glove_vectors.keys():\n",
        "        vec = glove_vectors[word]\n",
        "        embedding_matrix[i] = vec\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "BATCH_SIZE = 14\n",
        "\n",
        "\n",
        "\n",
        "def load_image(id_, report):\n",
        "    '''Loads the Image Features with their corresponding Ids'''\n",
        "    img_feature = Xnet_Features[id_.decode('utf-8')][0]\n",
        "    return img_feature, report\n",
        "\n",
        "\n",
        "def create_dataset(img_name, caption):\n",
        "  \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_name, caption))\n",
        "\n",
        "  # Use map to load the numpy files in parallel\n",
        "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(load_image, [item1, item2], [tf.float32, tf.string]),\n",
        "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Shuffle and batch\n",
        "    dataset = dataset.shuffle(500).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "train_generator = create_dataset(X_train, y_train)\n",
        "cv_generator = create_dataset(X_cv, y_cv)\n",
        "\n",
        "def bytes_to_string(arr):\n",
        "    '''The generator gives provides data in bytes. This function converts them back to strings for manipulation'''\n",
        "    for i in range(len(arr)):\n",
        "        arr[i] = arr[i].decode('utf-8')\n",
        "    return arr\n",
        "\n",
        "\n",
        "input1 = Input(shape=(2048), name='Image_1')\n",
        "dense1 = Dense(256, kernel_initializer=tf.keras.initializers.glorot_uniform(seed = 56), name='dense_encoder')(input1)\n",
        "\n",
        "input2 = Input(shape=(153), name='Text_Input')\n",
        "emb_layer = Embedding(input_dim = vocab_size, output_dim = 300, input_length=153, mask_zero=True, trainable=False, \n",
        "                weights=[embedding_matrix], name=\"Embedding_layer\")\n",
        "emb = emb_layer(input2)\n",
        "\n",
        "LSTM1 = LSTM(units=256, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n",
        "            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=23),\n",
        "            recurrent_initializer=tf.keras.initializers.orthogonal(seed=7),\n",
        "            bias_initializer=tf.keras.initializers.zeros(), return_sequences=True, name=\"LSTM1\")(emb)\n",
        "#LSTM1_output = LSTM1(emb)\n",
        "\n",
        "LSTM2 = LSTM(units=256, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n",
        "            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=23),\n",
        "            recurrent_initializer=tf.keras.initializers.orthogonal(seed=7),\n",
        "            bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM2\")\n",
        "LSTM2_output = LSTM2(LSTM1)\n",
        "\n",
        "dropout1 = Dropout(0.5, name='dropout1')(LSTM2_output)\n",
        "\n",
        "dec =  tf.keras.layers.Add()([dense1, dropout1])\n",
        "\n",
        "fc1 = Dense(256, activation='relu', kernel_initializer=tf.keras.initializers.he_normal(seed = 63), name='fc1')\n",
        "fc1_output = fc1(dec)\n",
        "dropout2 = Dropout(0.4, name='dropout2')(fc1_output)\n",
        "output_layer = Dense(vocab_size, activation='softmax', name='Output_layer')\n",
        "output = output_layer(dropout2)\n",
        "\n",
        "encoder_decoder = Model(inputs = [input1, input2], outputs = output)\n",
        "#encoder_decoder.summary()\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')\n",
        "\n",
        "def maskedLoss(y_true, y_pred):\n",
        "    #getting mask value\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "    \n",
        "    #calculating the loss\n",
        "    loss_ = loss_function(y_true, y_pred)\n",
        "    \n",
        "    #converting mask dtype to loss_ dtype\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    \n",
        "    #applying the mask to loss\n",
        "    loss_ = loss_*mask\n",
        "    \n",
        "    #getting mean over all the values\n",
        "    loss_ = tf.reduce_mean(loss_)\n",
        "    return loss_\n",
        "\n",
        "\n",
        "encoder_decoder.compile(optimizer, loss = maskedLoss)\n",
        "\n",
        "\n",
        "encoder_decoder.load_weights('/content/drive/MyDrive/FYP2//encoder_decoder_epoch_240.h5')\n",
        "\n",
        "\n",
        "\n",
        "# encoder\n",
        "encoder_input = encoder_decoder.input[0]\n",
        "encoder_output = encoder_decoder.get_layer('dense_encoder').output\n",
        "encoder_model = Model(encoder_input, encoder_output)\n",
        "\n",
        "# decoder# \n",
        "text_input = encoder_decoder.input[1]\n",
        "enc_output = Input(shape=(256,), name='Enc_Output')\n",
        "text_output = encoder_decoder.get_layer('LSTM2').output\n",
        "add1 = tf.keras.layers.Add()([text_output, enc_output])\n",
        "fc_1 = fc1(add1)\n",
        "decoder_output = output_layer(fc_1)\n",
        "\n",
        "decoder_model = Model(inputs = [text_input, enc_output], outputs = decoder_output)\n",
        "\n",
        "\n",
        "\n",
        "def load_image1(img_name):\n",
        "  image = Image.open(img_name)\n",
        "  image_array = np.asarray(image.convert(\"RGB\"))\n",
        "  image_array = image_array / 255.\n",
        "  image_array = resize(image_array, (224,224))\n",
        "  X = np.expand_dims(image_array, axis=0)\n",
        "  X = np.asarray(X) \n",
        "  return X\n",
        "\n",
        "\n",
        "\n",
        "def beamsearch(image):\n",
        "    \n",
        "    start = [tokenizer.word_index['startseq']]\n",
        "    print(type(image))\n",
        "    sequences = [[start, 0]]\n",
        "    beam_width = 5\n",
        "    #i1 = load_image1(image)\n",
        "    img1_features = chexnet.predict(image)    \n",
        "    #i2 = load_image1(image)\n",
        "    img2_features = chexnet.predict(image)\n",
        "    input_ = np.concatenate((img1_features, img2_features), axis=1)\n",
        "    img_features =  input_\n",
        "    img_features = encoder_model.predict(img_features)\n",
        "    finished_seq = []\n",
        "    \n",
        "    for i in range(153):\n",
        "        all_candidates = []\n",
        "        new_seq = []\n",
        "        for s in sequences:\n",
        "\n",
        "            text_input = pad_sequences([s[0]], 153, padding='post')\n",
        "            predictions = decoder_model.predict([text_input,img_features])\n",
        "            top_words = np.argsort(predictions[0])[-beam_width:]\n",
        "            seq, score = s\n",
        "            \n",
        "            for t in top_words:\n",
        "                candidates = [seq + [t], score - np.log(predictions[0][t])]\n",
        "                all_candidates.append(candidates)\n",
        "                \n",
        "        sequences = sorted(all_candidates, key = lambda l: l[1])[:beam_width]\n",
        "        # checks for 'endseq' in each seq in the beam\n",
        "        count = 0\n",
        "        for seq,score in sequences:\n",
        "            if seq[len(seq)-1] == tokenizer.word_index['endseq']:\n",
        "                score = score/len(seq)   # normalized\n",
        "                finished_seq.append([seq, score])\n",
        "                count+=1\n",
        "            else:\n",
        "                new_seq.append([seq, score])\n",
        "        beam_width -= count\n",
        "        sequences = new_seq\n",
        "        \n",
        "        # if all the sequences reaches its end before 155 timesteps\n",
        "        if not sequences:\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "    sequences = finished_seq[-1] \n",
        "    rep = sequences[0]\n",
        "    score = sequences[1]\n",
        "    temp = []\n",
        "    rep.pop(0)\n",
        "    for word in rep:\n",
        "        if word != tokenizer.word_index['endseq']:\n",
        "            temp.append(tokenizer.index_word[word])\n",
        "        else:\n",
        "            break    \n",
        "    rep = ' '.join(e for e in temp)        \n",
        "    return rep\n",
        "\n"
      ],
      "metadata": {
        "id": "Za5mIcPAg9o0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PhBe4dkCqeKX"
      },
      "outputs": [],
      "source": [
        "app = FastAPI()\n",
        "\n",
        "origins = ['*']\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins = origins,\n",
        "    allow_credentials = True,\n",
        "    allow_methods = ['*'],\n",
        "    allow_headers = ['*'],\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_ZJlFsq-qmtf"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@app.post('/')\n",
        "async def root(file:bytes=File(...)):\n",
        "    # print(request)\n",
        "    #file_contents = await file.read()\n",
        "    #print('Content: ',file_contents)\n",
        "    #print('STRING', file)\n",
        "    image = Image.open(io.BytesIO(file))\n",
        "    image_array = np.asarray(image.convert(\"RGB\"))\n",
        "    image_array = image_array / 255.\n",
        "    image_array = resize(image_array, (224,224))\n",
        "    X = np.expand_dims(image_array, axis=0)\n",
        "    X = np.asarray(X)\n",
        "    print(type(X))\n",
        "    #plt.imshow(image)\n",
        "    res0 = beamsearch(X)\n",
        "    return res0\n",
        "    \n",
        "    # from google.colab import files\n",
        "    # files.download('result_test_256.obj')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GcLv4i4qpas",
        "outputId": "daa7aa18-1bd8-4257-8896-40b5c3087c89"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: http://95d4-34-150-234-228.ngrok.io\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [117]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "INFO:     2407:aa80:314:a6e3:51de:d622:86c0:2e72:0 - \"POST / HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [117]\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kID5o9mr-th"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}